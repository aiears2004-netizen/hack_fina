{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e15713ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aiear_vpvq6b8\\Desktop\\pro\\hack\\hask\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "248a1657",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e725886",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_DIM = 384  # embedding size for the above model\n",
    "MIN_SIMILARITY = 0.82  # dynamic threshold (best performance)\n",
    "TOP_K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e8869d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatIP(VECTOR_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "394b0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_CACHE = FAISS(\n",
    "    embedding_function=EMBED,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c57a27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no data\n"
     ]
    }
   ],
   "source": [
    "if QA_CACHE.index.ntotal == 0:\n",
    "    print(\"no data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c41536f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aiear_vpvq6b8\\Desktop\\pro\\hack\\hask\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3:12b\",\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09ed30c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"]=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm_az = init_chat_model(\n",
    "    \"azure_openai:gpt-4.1\",\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    ")\n",
    "# llm_az.invoke(\"hi how are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5621af18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class topicseg(TypedDict):\n",
    "    topic:str =Field(\n",
    "        ...,\n",
    "        description=\"You are a topic segregation assistant. Your task is to analyze each user query and categorize it into an appropriate topic\",\n",
    "    )\n",
    "    answer: str = Field(...,\n",
    "                        description=\"give me a correct answer for user question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8725d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(topicseg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef358eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"who is chief minister?\"\n",
    "res=structured_llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5280e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dcef3b46-a5d1-48f3-a39f-147394aa5155']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA_CACHE.add_texts(\n",
    "        texts=[question],\n",
    "        metadatas=[{\n",
    "            \"topic\": res[\"topic\"],\n",
    "            \"answer\": res[\"answer\"],\n",
    "        }]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73b7de27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1a43009d150>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9592a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1=\"who is chief minister of Andhra Pradesh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c0097b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = QA_CACHE.similarity_search_with_score(question1, k=TOP_K)\n",
    "best_doc, dist = results[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "947083e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Chief Minister',\n",
       " 'answer': \"The current Chief Minister of India varies depending on the state. Here's a breakdown as of today, May 16, 2024:\\n\\n*   **Andhra Pradesh:** N. Chandrababu Naidu\\n*   **Arunachal Pradesh:** Pema Khandu\\n*   **Assam:** Himanta Biswa Sarma\\n*   **Bihar:** Nitish Kumar\\n*   **Chhattisgarh:** Vishnu Deo Sai\\n*   **Goa:** Pramod Sawant\\n*   **Gujarat:** Bhupendra Patel\\n*   **Haryana:** Manohar Lal Khattar (resigned, new CM to be appointed soon)\\n*   **Himachal Pradesh:** Sukhvinder Singh Sukhu\\n*   **Jharkhand:** Hemant Soren (currently under arrest, interim CM appointed)\\n*   **Karnataka:** Siddaramaiah\\n*   **Kerala:** Pinarayi Vijayan\\n*   **Madhya Pradesh:** Mohan Yadav\\n*   **Maharashtra:** Eknath Shinde\\n*   **Manipur:** N. Biren Singh\\n*   **Meghalaya:** Conrad K. Sangma\\n*   **Mizoram:** Lal Thangwala\\n*   **Nagaland:** Nephiu Rio\\n*   **Odisha:** Naveen Patnaik\\n*   **Punjab:** Bhagwant Mann\\n*   **Rajasthan:** Bhajan Lal Sharma\\n*   **Sikkim:** Prem Singh Tamang\\n*   **Tamil Nadu:** M. K. Stalin\\n*   **Telangana:** A. Revanth Reddy\\n*   **Tripura:** Manik Saha\\n*   **Uttar Pradesh:** Yogi Adityanath\\n*   **Uttarakhand:** Pushkar Singh Dhami\\n*   **West Bengal:** Mamata Banerjee\\n\\n**Note:** Political situations can change rapidly. It's always a good idea to double-check with a reliable news source for the very latest information.\"}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_doc.metadata\n",
    "# dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a01297d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: be1b3eb5-dc41-47cc-90bf-9bc7b71cf41e\n",
      "Text: tell me answer 2+2\n",
      "Metadata: mathematics\n",
      "answer: 2 + 2 = 4\n",
      "--------------------------------------------------\n",
      "ID: 63128617-f966-45c0-836f-1cb7dd82bdc6\n",
      "Text: tell me about prime minister in 20 words\n",
      "Metadata: Prime Minister\n",
      "answer: The Prime Minister leads the government, typically the leader of the majority party, responsible for policy and representing the nation.\n",
      "--------------------------------------------------\n",
      "ID: 34e6408e-660e-4732-9fd1-52d447202cc6\n",
      "Text: tell me about prime minister in 20 words\n",
      "Metadata: Prime Minister\n",
      "answer: The Prime Minister leads the government, typically the leader of the majority party, responsible for policy and representing the nation.\n",
      "--------------------------------------------------\n",
      "ID: dcef3b46-a5d1-48f3-a39f-147394aa5155\n",
      "Text: who is chief minister?\n",
      "Metadata: Chief Minister\n",
      "answer: The current Chief Minister of India varies depending on the state. Here's a breakdown as of today, May 16, 2024:\n",
      "\n",
      "*   **Andhra Pradesh:** N. Chandrababu Naidu\n",
      "*   **Arunachal Pradesh:** Pema Khandu\n",
      "*   **Assam:** Himanta Biswa Sarma\n",
      "*   **Bihar:** Nitish Kumar\n",
      "*   **Chhattisgarh:** Vishnu Deo Sai\n",
      "*   **Goa:** Pramod Sawant\n",
      "*   **Gujarat:** Bhupendra Patel\n",
      "*   **Haryana:** Manohar Lal Khattar (resigned, new CM to be appointed soon)\n",
      "*   **Himachal Pradesh:** Sukhvinder Singh Sukhu\n",
      "*   **Jharkhand:** Hemant Soren (currently under arrest, interim CM appointed)\n",
      "*   **Karnataka:** Siddaramaiah\n",
      "*   **Kerala:** Pinarayi Vijayan\n",
      "*   **Madhya Pradesh:** Mohan Yadav\n",
      "*   **Maharashtra:** Eknath Shinde\n",
      "*   **Manipur:** N. Biren Singh\n",
      "*   **Meghalaya:** Conrad K. Sangma\n",
      "*   **Mizoram:** Lal Thangwala\n",
      "*   **Nagaland:** Nephiu Rio\n",
      "*   **Odisha:** Naveen Patnaik\n",
      "*   **Punjab:** Bhagwant Mann\n",
      "*   **Rajasthan:** Bhajan Lal Sharma\n",
      "*   **Sikkim:** Prem Singh Tamang\n",
      "*   **Tamil Nadu:** M. K. Stalin\n",
      "*   **Telangana:** A. Revanth Reddy\n",
      "*   **Tripura:** Manik Saha\n",
      "*   **Uttar Pradesh:** Yogi Adityanath\n",
      "*   **Uttarakhand:** Pushkar Singh Dhami\n",
      "*   **West Bengal:** Mamata Banerjee\n",
      "\n",
      "**Note:** Political situations can change rapidly. It's always a good idea to double-check with a reliable news source for the very latest information.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for doc_id, doc in QA_CACHE.docstore._dict.items():\n",
    "    print(\"ID:\", doc_id)\n",
    "    print(\"Text:\", doc.page_content)\n",
    "    print(\"Metadata:\", doc.metadata[\"topic\"])\n",
    "    print(\"answer:\", doc.metadata[\"answer\"])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "400a1740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class TopicSeg(TypedDict):\n",
    "    topic: str\n",
    "    answer: str\n",
    "\n",
    "structured_llm = llm_az.with_structured_output(TopicSeg)\n",
    "\n",
    "\n",
    "TOPIC_INDEX = FAISS(\n",
    "    embedding_function=EMBED,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "def resolve_topic(raw_topic):\n",
    "\n",
    "    global TOPIC_INDEX\n",
    "\n",
    "    # 1️⃣ If this is the very first topic → create FAISS index fully\n",
    "    if TOPIC_INDEX is None:\n",
    "        TOPIC_INDEX = FAISS.from_texts(\n",
    "            texts=[raw_topic],\n",
    "            embedding=EMBED,\n",
    "            metadatas=[{\"similarity\": 1.0}]\n",
    "        )\n",
    "        return raw_topic\n",
    "\n",
    "    # 2️⃣ Search for nearest topic\n",
    "    topic_results = TOPIC_INDEX.similarity_search_with_score(raw_topic, k=1)\n",
    "    doc, score = topic_results[0]\n",
    "\n",
    "    # 3️⃣ If too different → add new topic to same index\n",
    "    if score > 0.30:  # Distance threshold\n",
    "        TOPIC_INDEX.add_texts(\n",
    "            texts=[raw_topic],\n",
    "            metadatas=[{\"similarity\": 1.0}]\n",
    "        )\n",
    "        return raw_topic\n",
    "\n",
    "    # 4️⃣ Else → re-map to nearest existing topic\n",
    "    return doc.page_content\n",
    "\n",
    "\n",
    "\n",
    "def store_to_memory(question, topic, answer):\n",
    "    QA_CACHE.add_texts(\n",
    "        texts=[question],\n",
    "        metadatas=[{\n",
    "            \"topic\": topic,\n",
    "            \"answer\": answer,\n",
    "        }]\n",
    "    )\n",
    "def retrieve_memory(query, topic, k=3):\n",
    "    results = QA_CACHE.similarity_search_with_score(query, k=k)\n",
    "\n",
    "    # Filter by topic automatically (no manual comparison needed)\n",
    "    filtered = [\n",
    "        (doc, score)\n",
    "        for doc, score in results\n",
    "        if doc.metadata.get(\"topic\") == topic\n",
    "    ]\n",
    "    return filtered\n",
    "\n",
    "def answer_question(question):\n",
    "    # Step 1: LLM extracts topic + rough answer\n",
    "    res = structured_llm.invoke(question)\n",
    "    raw_topic = res[\"topic\"]\n",
    "\n",
    "    # Step 2: Map automatically to best existing topic\n",
    "    topic = resolve_topic(raw_topic)\n",
    "\n",
    "    # Step 3: Fetch memory from that topic\n",
    "    history = retrieve_memory(question, topic)\n",
    "\n",
    "    context = \"\\n\".join(d.metadata['answer'] for d, s in history)\n",
    "\n",
    "    final_prompt = f\"\"\"\n",
    "    Topic: {topic}\n",
    "    Relevant History:\n",
    "    {context}\n",
    "\n",
    "    User Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    final_answer = llm_az.invoke(final_prompt)\n",
    "\n",
    "    # Step 4: Store memory under topic cluster\n",
    "    store_to_memory(question, topic, final_answer)\n",
    "\n",
    "    return final_answer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c66f6a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer_question(\"Who is the Prime Minister of India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65b66803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Imports\n",
    "# ---------------------------------------------------------\n",
    "from typing import List, Tuple\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_core.documents import Document\n",
    "import faiss\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Assumptions: you ALREADY have these defined in your code\n",
    "# ---------------------------------------------------------\n",
    "# llm_az : your chat LLM (e.g., Azure OpenAI ChatModel)\n",
    "# EMBED  : your embedding model (must implement .embed_query and .embed_documents)\n",
    "\n",
    "# Example (JUST for reference, don't duplicate if you already have them):\n",
    "# from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "# llm_az = AzureChatOpenAI(...)\n",
    "# EMBED = AzureOpenAIEmbeddings(...)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Structured output schema for topic + answer\n",
    "# ---------------------------------------------------------\n",
    "class TopicSeg(TypedDict):\n",
    "    topic: str\n",
    "    answer: str\n",
    "\n",
    "# LLM that returns { \"topic\": \"...\", \"answer\": \"...\" }\n",
    "structured_llm = llm_az.with_structured_output(TopicSeg)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. QA Cache Vector Store (for questions + answers + topic)\n",
    "# ---------------------------------------------------------\n",
    "# Build an empty FAISS index for QA cache using the real embedding dimension\n",
    "dummy_emb = EMBED.embed_query(\"dimension test\")\n",
    "embedding_dim = len(dummy_emb)\n",
    "\n",
    "qa_faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "QA_CACHE = FAISS(\n",
    "    embedding_function=EMBED,\n",
    "    index=qa_faiss_index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Topic Index (for clustering topics themselves)\n",
    "# ---------------------------------------------------------\n",
    "# IMPORTANT: start as None, will be initialized on first topic\n",
    "TOPIC_INDEX = None  # type: FAISS | None\n",
    "\n",
    "\n",
    "def resolve_topic(raw_topic: str, distance_threshold: float = 0.30) -> str:\n",
    "    \"\"\"\n",
    "    Map a raw topic string to an existing topic cluster if similar,\n",
    "    otherwise create a new topic in the topic index.\n",
    "    Uses L2 distance: smaller = more similar.\n",
    "    \"\"\"\n",
    "    global TOPIC_INDEX\n",
    "\n",
    "    # 1️⃣ First-ever topic → initialize topic FAISS index\n",
    "    if TOPIC_INDEX is None:\n",
    "        TOPIC_INDEX = FAISS.from_texts(\n",
    "            texts=[raw_topic],\n",
    "            embedding=EMBED,\n",
    "            metadatas=[{\"similarity\": 1.0}]\n",
    "        )\n",
    "        # raw_topic is both the label and the first cluster center\n",
    "        return raw_topic\n",
    "\n",
    "    # 2️⃣ Topic index already exists → search nearest topic\n",
    "    results = TOPIC_INDEX.similarity_search_with_score(raw_topic, k=1)\n",
    "\n",
    "    doc, distance = results[0]  # distance = L2 distance from topic embedding\n",
    "\n",
    "    # 3️⃣ If too far → treat as new topic cluster\n",
    "    # (you can tune distance_threshold)\n",
    "    if distance > distance_threshold:\n",
    "        TOPIC_INDEX.add_texts(\n",
    "            texts=[raw_topic],\n",
    "            metadatas=[{\"similarity\": 1.0}]\n",
    "        )\n",
    "        return raw_topic\n",
    "\n",
    "    # 4️⃣ Close enough → reuse existing mapped topic name\n",
    "    return doc.page_content\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Memory Store and Retrieval\n",
    "# ---------------------------------------------------------\n",
    "def store_to_memory(question: str, topic: str, answer: str) -> None:\n",
    "    \"\"\"\n",
    "    Store question + topic + answer in QA_CACHE.\n",
    "    \"\"\"\n",
    "    QA_CACHE.add_texts(\n",
    "        texts=[question],\n",
    "        metadatas=[{\n",
    "            \"topic\": topic,\n",
    "            \"answer\": answer,\n",
    "        }]\n",
    "    )\n",
    "\n",
    "\n",
    "def retrieve_memory(query: str, topic: str, k: int = 3) -> List[Tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    Retrieve top-k similar QAs for this query, filtered to the same topic.\n",
    "    Returns list of (Document, score).\n",
    "    \"\"\"\n",
    "    # If QA cache is empty, nothing to retrieve\n",
    "    if QA_CACHE.index.ntotal == 0:\n",
    "        return []\n",
    "\n",
    "    results = QA_CACHE.similarity_search_with_score(query, k=k)\n",
    "\n",
    "    # Filter only docs matching this topic (topic-aware RAG)\n",
    "    filtered = [\n",
    "        (doc, score)\n",
    "        for doc, score in results\n",
    "        if doc.metadata.get(\"topic\") == topic\n",
    "    ]\n",
    "    return filtered\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. Main pipeline: answer_question()\n",
    "# ---------------------------------------------------------\n",
    "def answer_question(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Full flow:\n",
    "      1) Use structured LLM to get {topic, rough answer}\n",
    "      2) Resolve topic to an existing cluster (or create new)\n",
    "      3) Retrieve history for that topic\n",
    "      4) Ask final LLM with history as context\n",
    "      5) Store final answer in memory under that topic\n",
    "    \"\"\"\n",
    "    # Step 1: LLM extracts topic + rough answer (not final yet)\n",
    "    res: TopicSeg = structured_llm.invoke(question)\n",
    "    raw_topic = res[\"topic\"]\n",
    "    rough_answer = res[\"answer\"]  # you can use this as a fallback if you want\n",
    "\n",
    "    # Step 2: Map automatically to best existing topic cluster\n",
    "    topic = resolve_topic(raw_topic)\n",
    "\n",
    "    # Step 3: Fetch memory from that topic\n",
    "    history = retrieve_memory(question, topic, k=5)\n",
    "\n",
    "    # Build history text block\n",
    "    if history:\n",
    "        history_block = \"\\n\".join(\n",
    "            f\"- Q: {doc.page_content}\\n  A: {doc.metadata.get('answer', '')}\"\n",
    "            for doc, _ in history\n",
    "        )\n",
    "    else:\n",
    "        history_block = \"None yet.\"\n",
    "\n",
    "    # Step 4: Ask final LLM with topic + history context\n",
    "    final_prompt = f\"\"\"\n",
    "You are a helpful assistant.\n",
    "\n",
    "Topic: {topic}\n",
    "\n",
    "Previous Q&A for this topic:\n",
    "{history_block}\n",
    "\n",
    "User question: {question}\n",
    "\n",
    "Using the above history only if it is relevant, give the best possible answer.\n",
    "Be accurate and don't hallucinate details you don't know.\n",
    "\"\"\"\n",
    "\n",
    "    final_answer_obj = llm_az.invoke(final_prompt)\n",
    "\n",
    "    # LangChain ChatModels often return an object with `.content`\n",
    "    try:\n",
    "        final_answer = final_answer_obj.content\n",
    "    except AttributeError:\n",
    "        final_answer = str(final_answer_obj)\n",
    "\n",
    "    # Step 5: Store final answer in memory\n",
    "    store_to_memory(question, topic, final_answer)\n",
    "\n",
    "    return final_answer\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. Simple Testing\n",
    "# ---------------------------------------------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     q1 = \"Who is the Prime Minister of India?\"\n",
    "#     print(\"Q1:\", q1)\n",
    "#     a1 = answer_question(q1)\n",
    "#     print(\"A1:\", a1, \"\\n\")\n",
    "\n",
    "#     q2 = \"Which party is currently ruling India?\"\n",
    "#     print(\"Q2:\", q2)\n",
    "#     a2 = answer_question(q2)\n",
    "#     print(\"A2:\", a2, \"\\n\")\n",
    "\n",
    "#     q3 = \"Who is the Chief Minister of Tamil Nadu?\"\n",
    "#     print(\"Q3:\", q3)\n",
    "#     a3 = answer_question(q3)\n",
    "#     print(\"A3:\", a3, \"\\n\")\n",
    "\n",
    "#     # You can also inspect what's in QA_CACHE:\n",
    "#     print(\"---- Stored QA CACHE metadata ----\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25367064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It seems that there is no prior code provided in this discussion thread. Could you please share the code you'd like me to check and correct? Once I have the code, I'll help you debug and fix any issues.\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(\"the code u gave previously is not working check and crt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b41d5eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Political Leadership in India' metadata={'similarity': 1.0}\n",
      "0.77778465\n"
     ]
    }
   ],
   "source": [
    "raw_topic=\"Leadership\"\n",
    "results = TOPIC_INDEX.similarity_search_with_score(raw_topic, k=1)\n",
    "doc,distance = results[0]\n",
    "print(doc)\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91dbd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4774078e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 744fcda4-1598-47cc-8fab-d22fe383882f\n",
      "Text: Which party is ruling India?\n",
      "Metadata: {'topic': 'Political Leadership in India', 'answer': \"The Bharatiya Janata Party (BJP) is currently the ruling party in India. The party has been in power at the central government since 2014, with Prime Minister Narendra Modi serving as the leader. The BJP won consecutive elections in 2014 and 2019, securing its majority in the Lok Sabha, India's lower house of Parliament.\"}\n",
      "----------------------------------------\n",
      "ID: 9350e344-4b25-4936-a7a1-5fbb28201e6e\n",
      "Text: How to unlock laptop without password?\n",
      "Metadata: {'topic': 'Laptop Password Recovery', 'answer': 'If you need to unlock a laptop without a password, the best approach depends on the specific situation and the operating system being used. Below are general methods:\\n\\n### For Windows:\\n1. **Use a Password Reset Disk or Security Questions**: If you set up a password reset disk or security questions when creating your account, you can use those to reset your password.\\n2. **Reset via Microsoft Account**: If your laptop is connected to a Microsoft account, go to the Microsoft account password reset page on another device and follow the steps to reset your password, then sign in with the updated password on the laptop.\\n3. **Use Safe Mode**: Restart the laptop in Safe Mode and log in as an administrator (if applicable). From there, you can potentially reset the password of the locked account.\\n4. **Factory Reset**: You can reset the laptop to its factory settings, which will remove the password but also erase all data. Look for recovery options typically available during startup (e.g., pressing F8, F11, or a specific key for your laptop manufacturer).\\n5. **Contact Support or Manufacturer**: If none of these options work, contact your laptop manufacturer or support team for assistance. \\n\\n### For macOS:\\n1. **Use Apple ID**: If your laptop is linked to your Apple ID, you can reset the password using the instructions on the login screen.\\n2. **Recovery Mode**: Restart the laptop and boot into Recovery Mode by holding Command (⌘) + R during startup. From here, you can use the Password Reset tool.\\n3. **Contact Apple Support**: If you don’t have access to your Apple ID, contact Apple for assistance.\\n\\n---\\n\\n**Important Notes:**\\n- Verify ownership of the laptop before proceeding with any recovery actions.\\n- Be cautious when attempting to bypass passwords. Missteps could lead to data loss or system access restrictions.'}\n",
      "----------------------------------------\n",
      "ID: 7f1edfcc-0337-467d-af96-c0f83961518b\n",
      "Text: Pandas dataframe drop columns\n",
      "Metadata: {'topic': 'Pandas DataFrame Manipulation', 'answer': 'To drop columns in a Pandas DataFrame, you can use the `.drop()` method. Here\\'s how you can do it:\\n\\n### Syntax\\n\\n```python\\ndf.drop(columns=[\\'column_name1\\', \\'column_name2\\'], inplace=False)\\n```\\n\\n### Key Parameters\\n1. **`columns`**: This specifies the column(s) you want to drop. You can pass a single column name as a string or multiple column names as a list of strings.\\n2. **`inplace`**: Set this to `True` to modify the DataFrame directly. If set to `False` (default), a modified copy of the DataFrame is returned.\\n\\n### Example\\n\\n```python\\nimport pandas as pd\\n\\n# Create a sample DataFrame\\ndata = {\\n    \\'A\\': [1, 2, 3],\\n    \\'B\\': [4, 5, 6],\\n    \\'C\\': [7, 8, 9],\\n}\\ndf = pd.DataFrame(data)\\n\\nprint(\"Original DataFrame:\")\\nprint(df)\\n\\n# Dropping the column \\'B\\'\\ndf_dropped = df.drop(columns=[\\'B\\'])\\n\\nprint(\"\\\\nDataFrame after dropping column \\'B\\':\")\\nprint(df_dropped)\\n\\n# Dropping multiple columns (\\'A\\' and \\'C\\') with inplace=True\\ndf.drop(columns=[\\'A\\', \\'C\\'], inplace=True)\\n\\nprint(\"\\\\nDataFrame after dropping columns \\'A\\' and \\'C\\' with inplace=True:\")\\nprint(df)\\n```\\n\\n### Output:\\n```\\nOriginal DataFrame:\\n   A  B  C\\n0  1  4  7\\n1  2  5  8\\n2  3  6  9\\n\\nDataFrame after dropping column \\'B\\':\\n   A  C\\n0  1  7\\n1  2  8\\n2  3  9\\n\\nDataFrame after dropping columns \\'A\\' and \\'C\\' with inplace=True:\\n   B\\n0  4\\n1  5\\n2  6\\n```\\n\\n### Notes:\\n- Be cautious with `inplace=True` because once used, the DataFrame is modified directly and you may lose information about the dropped columns.\\n- The `.drop()` method is widely used for column and row removal, so if you need to drop rows instead of columns, you can use the `index` parameter in place of `columns`.'}\n",
      "----------------------------------------\n",
      "ID: 1dbf9f50-6454-473d-bb73-07f8ebb48440\n",
      "Text: Chief Minister of Tamil Nadu?\n",
      "Metadata: {'topic': 'Politics', 'answer': 'As of October 2023, the Chief Minister of Tamil Nadu is **M.K. Stalin**. He is the leader of the Dravida Munnetra Kazhagam (DMK) party and has been serving as Chief Minister since May 2021.'}\n",
      "----------------------------------------\n",
      "ID: 0f20f699-5c25-4756-a6b0-6f39afbc5f7e\n",
      "Text: Write code for preprocessing\n",
      "Metadata: {'topic': 'Data Preprocessing', 'answer': 'Certainly! Data preprocessing is an essential step before using data for analysis, modeling, or visualization. Here\\'s an example of Python code that demonstrates basic data preprocessing using the `pandas` library:\\n\\n```python\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\\n\\n# Example: Load the dataset\\ndata = pd.DataFrame({\\n    \\'Name\\': [\\'Alice\\', \\'Bob\\', \\'Charlie\\', None],\\n    \\'Age\\': [25, 30, None, 40],\\n    \\'Salary\\': [50000, 60000, 55000, None],\\n    \\'City\\': [\\'New York\\', \\'Los Angeles\\', \\'Chicago\\', \\'New York\\'],\\n    \\'Purchased\\': [\\'Yes\\', \\'No\\', \\'Yes\\', None]\\n})\\n\\nprint(\"Original Data:\")\\nprint(data)\\n\\n# Step 1: Handle missing values\\n# Fill numerical columns with mean and categorical columns with mode\\ndata[\\'Age\\'] = data[\\'Age\\'].fillna(data[\\'Age\\'].mean())\\ndata[\\'Salary\\'] = data[\\'Salary\\'].fillna(data[\\'Salary\\'].mean())\\ndata[\\'Name\\'] = data[\\'Name\\'].fillna(\\'Unknown\\')\\ndata[\\'Purchased\\'] = data[\\'Purchased\\'].fillna(data[\\'Purchased\\'].mode()[0])\\n\\nprint(\"\\\\nData after handling missing values:\")\\nprint(data)\\n\\n# Step 2: Encode categorical columns\\n# Example: Encoding \\'Purchased\\' column as \\'Yes\\' -> 1, \\'No\\' -> 0\\nlabel_encoder = LabelEncoder()\\ndata[\\'Purchased\\'] = label_encoder.fit_transform(data[\\'Purchased\\'])\\n\\n# For one-hot encoding, you can also use pd.get_dummies() on \\'City\\'\\ndata = pd.get_dummies(data, columns=[\\'City\\'], drop_first=True)\\n\\nprint(\"\\\\nData after encoding categorical columns:\")\\nprint(data)\\n\\n# Step 3: Scale numerical features\\n# Standardize \\'Age\\' and \\'Salary\\' to have mean=0 and std deviation=1\\nscaler = StandardScaler()\\ndata[[\\'Age\\', \\'Salary\\']] = scaler.fit_transform(data[[\\'Age\\', \\'Salary\\']])\\n\\nprint(\"\\\\nData after scaling numerical features:\")\\nprint(data)\\n\\n# Step 4: Split the dataset into training and testing sets\\n# Assume \\'Purchased\\' column is the target variable\\nX = data.drop(columns=[\\'Purchased\\'])  # Features\\ny = data[\\'Purchased\\']  # Target\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\nprint(\"\\\\nTraining and testing data:\")\\nprint(\"X_train:\")\\nprint(X_train)\\nprint(\"\\\\ny_train:\")\\nprint(y_train)\\n```\\n\\n### Explanation of the Code:\\n1. **Handling Missing Values**:\\n   - Numerical columns (`Age`, `Salary`) are filled with their mean.\\n   - Categorical columns (`Name`, `Purchased`) are filled with their mode or a placeholder value.\\n\\n2. **Encoding Categorical Data**:\\n   - Label encoding is applied to the `Purchased` column.\\n   - One-hot encoding is used for the `City` column to create indicator variables.\\n\\n3. **Feature Scaling**:\\n   - Standardization is applied to numerical features (`Age`, `Salary`) using `StandardScaler`.\\n\\n4. **Train-Test Split**:\\n   - The dataset is split into training and testing sets to evaluate the model later.\\n\\nThis is a general workflow, and you can customize it based on your specific dataset and requirements. Let me know if you need help with a specific dataset or preprocessing method!'}\n",
      "----------------------------------------\n",
      "ID: 1e8ef546-ace7-42fc-a701-ca3dd025a1e4\n",
      "Text: my name is arun\n",
      "Metadata: {'topic': 'Introduction', 'answer': 'Nice to meet you, Arun! How can I assist you today?'}\n",
      "----------------------------------------\n",
      "ID: 44b6c449-6ea8-4727-a5c6-010ccb8e8926\n",
      "Text: what is my name\n",
      "Metadata: {'topic': 'Personal Identification', 'answer': \"I don't have access to any personal information about you, including your name, unless you explicitly share it with me in this conversation. If you'd like, you are welcome to share your name, and I'll be happy to address you accordingly!\"}\n",
      "----------------------------------------\n",
      "ID: ec0ad3b4-1eb2-4f66-bad5-42e20ddc3e4f\n",
      "Text: who is governing india right now\n",
      "Metadata: {'topic': 'Governance in India', 'answer': 'As of October 2023, India is governed by the Government of India, which operates as a parliamentary democratic republic. The current Prime Minister of India is **Narendra Modi**, who leads the central government. He is a member of the **Bharatiya Janata Party (BJP)**, which is the ruling party at the national level. The President of India, who serves as the ceremonial head of state, is **Droupadi Murmu**.'}\n",
      "----------------------------------------\n",
      "ID: 232b9548-c953-4211-b248-95fd71199d52\n",
      "Text: Which party is ruling India\n",
      "Metadata: {'topic': 'Politics', 'answer': 'As of October 2023, the **Bharatiya Janata Party (BJP)** is the ruling party in India. The BJP leads the central government under the National Democratic Alliance (NDA), and **Narendra Modi** serves as the Prime Minister of India.'}\n",
      "----------------------------------------\n",
      "ID: f3239702-8fd9-4cda-adf3-4e6570180ec3\n",
      "Text: my name is arun\n",
      "Metadata: {'topic': 'Introduction', 'answer': 'Nice to meet you again, Arun! How can I assist you today?'}\n",
      "----------------------------------------\n",
      "ID: d87502b1-fc62-4f42-bd41-0b94bb88b65e\n",
      "Text: what is my name\n",
      "Metadata: {'topic': 'Personalization Inquiry', 'answer': \"I don't have access to your name based on our current conversation. If you'd like to share it, feel free to let me know!\"}\n",
      "----------------------------------------\n",
      "ID: efa09f6e-bd20-4f0c-9d70-4c2ec7aa890e\n",
      "Text: what is my name\n",
      "Metadata: {'topic': 'Identity Inquiry', 'answer': \"I don’t know your name based on this conversation. If you'd like, you can share it or provide more context, and I’d be happy to assist further!\"}\n",
      "----------------------------------------\n",
      "ID: cbbbe7d4-34fd-457d-9bbd-4acbdd812d86\n",
      "Text: Write code for preprocessing\n",
      "Metadata: {'topic': 'Data Preprocessing', 'answer': 'Here’s an updated and relevant code snippet for data preprocessing. I’ve adapted the previous example, retaining only necessary elements based on what is typically needed to answer the question. Please note that preprocessing steps depend on the dataset and the specific requirements, but a general example is as follows:\\n\\n---\\n\\n```python\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\\n\\n# Example dataset\\ndata = pd.DataFrame({\\n    \\'Name\\': [\\'Alice\\', \\'Bob\\', \\'Charlie\\', None],\\n    \\'Age\\': [25, 30, None, 40],\\n    \\'Salary\\': [50000, 60000, 55000, None],\\n    \\'City\\': [\\'New York\\', \\'Los Angeles\\', \\'Chicago\\', \\'New York\\'],\\n    \\'Purchased\\': [\\'Yes\\', \\'No\\', \\'Yes\\', None]\\n})\\n\\n# Step 1: Handling Missing Values\\ndata[\\'Age\\'] = data[\\'Age\\'].fillna(data[\\'Age\\'].mean())\\ndata[\\'Salary\\'] = data[\\'Salary\\'].fillna(data[\\'Salary\\'].mean())\\ndata[\\'Name\\'] = data[\\'Name\\'].fillna(\\'Unknown\\')\\ndata[\\'Purchased\\'] = data[\\'Purchased\\'].fillna(data[\\'Purchased\\'].mode()[0])\\n\\n# Step 2: Encoding Categorical Data\\nlabel_encoder = LabelEncoder()\\ndata[\\'Purchased\\'] = label_encoder.fit_transform(data[\\'Purchased\\'])\\ndata = pd.get_dummies(data, columns=[\\'City\\'], drop_first=True)\\n\\n# Step 3: Feature Scaling\\nscaler = StandardScaler()\\ndata[[\\'Age\\', \\'Salary\\']] = scaler.fit_transform(data[[\\'Age\\', \\'Salary\\']])\\n\\n# Step 4: Splitting the Data into Training and Testing Sets\\nX = data.drop(columns=[\\'Purchased\\'])  # Features\\ny = data[\\'Purchased\\']  # Target\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Print data to verify preprocessing\\nprint(\"Preprocessed Data:\")\\nprint(data)\\nprint(\"\\\\nTraining Features (X_train):\")\\nprint(X_train)\\nprint(\"\\\\nTraining Labels (y_train):\")\\nprint(y_train)\\n```\\n\\n### Explanation:\\n1. **Handling Missing Values**:\\n   - Missing numerical values (`Age` and `Salary`) are filled with their mean.\\n   - Missing categorical values (`Name` and `Purchased`) are replaced with a placeholder (`Unknown`) or the mode.\\n\\n2. **Encoding Categorical Variables**:\\n   - `Purchased` column is label-encoded to numeric values (e.g., `Yes` → 1, `No` → 0).\\n   - One-hot encoding is applied for `City`, and the first column is dropped to avoid dummy variable traps.\\n\\n3. **Feature Scaling**:\\n   - Numerical columns (`Age`, `Salary`) are standardized using `StandardScaler`.\\n\\n4. **Train-Test Split**:\\n   - The dataset is split into training and testing sets, with a test size of 20%.\\n\\nThis workflow demonstrates how to process a dataset step-by-step before using it for analysis or modeling. Adjust the steps to fit your specific data requirements. Let me know if you have any follow-up questions!'}\n",
      "----------------------------------------\n",
      "ID: 8d12f7b0-67fe-4666-8e63-798bfa085c2f\n",
      "Text: Write code for preprocessing\n",
      "Metadata: {'topic': 'Data Preprocessing', 'answer': 'Sure! Below is a refined code snippet for data preprocessing, based on the shared history and general best practices for handling datasets. This code handles common steps such as missing values, encoding categorical variables, feature scaling, and splitting the data for further analysis. The code is designed to be modular and easily customizable.\\n\\n---\\n\\n```python\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\\n\\n# Example dataset\\ndata = pd.DataFrame({\\n    \\'Name\\': [\\'Alice\\', \\'Bob\\', \\'Charlie\\', None],\\n    \\'Age\\': [25, 30, None, 40],\\n    \\'Salary\\': [50000, 60000, 55000, None],\\n    \\'City\\': [\\'New York\\', \\'Los Angeles\\', \\'Chicago\\', \\'New York\\'],\\n    \\'Purchased\\': [\\'Yes\\', \\'No\\', \\'Yes\\', None]\\n})\\n\\n# Step 1: Handle missing values\\n# Fill numerical columns with the mean, categorical with mode or a placeholder\\ndata[\\'Age\\'] = data[\\'Age\\'].fillna(data[\\'Age\\'].mean())\\ndata[\\'Salary\\'] = data[\\'Salary\\'].fillna(data[\\'Salary\\'].mean())\\ndata[\\'Name\\'] = data[\\'Name\\'].fillna(\\'Unknown\\')\\ndata[\\'Purchased\\'] = data[\\'Purchased\\'].fillna(data[\\'Purchased\\'].mode()[0])\\n\\n# Step 2: Encode categorical variables\\n# Convert \\'Purchased\\' into numerical (e.g., Yes->1, No->0)\\nlabel_encoder = LabelEncoder()\\ndata[\\'Purchased\\'] = label_encoder.fit_transform(data[\\'Purchased\\'])\\n\\n# Apply one-hot encoding to \\'City\\' column\\ndata = pd.get_dummies(data, columns=[\\'City\\'], drop_first=True)\\n\\n# Step 3: Feature scaling\\n# Standardize numerical columns (\\'Age\\' and \\'Salary\\')\\nscaler = StandardScaler()\\ndata[[\\'Age\\', \\'Salary\\']] = scaler.fit_transform(data[[\\'Age\\', \\'Salary\\']])\\n\\n# Step 4: Split the data into training and testing sets\\n# Assume \\'Purchased\\' is the target variable\\nX = data.drop(columns=[\\'Purchased\\'])  # Features\\ny = data[\\'Purchased\\']  # Target column\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Output the processed data\\nprint(\"Preprocessed Dataset:\")\\nprint(data)\\nprint(\"\\\\nTraining Data (X_train):\")\\nprint(X_train)\\nprint(\"\\\\nTraining Labels (y_train):\")\\nprint(y_train)\\n```\\n\\n---\\n\\n### Explanation:\\n1. **Handling Missing Values**:\\n   - For numerical columns like `Age` and `Salary`, missing values are replaced with the column mean.\\n   - For categorical columns like `Name` and `Purchased`, missing values are handled with a placeholder (`Unknown`) or the mode of the column.\\n\\n2. **Encoding Categorical Variables**:\\n   - Label Encoding is applied to the `Purchased` column, converting it to `0` and `1`.\\n   - One-Hot Encoding is used for the `City` column, creating binary columns for each city (e.g., `City_Los Angeles`, `City_Chicago`).\\n\\n3. **Feature Scaling**:\\n   - Standardization is used to ensure numerical features (`Age`, `Salary`) have a mean of 0 and a standard deviation of 1, which is especially useful for models sensitive to feature magnitudes (e.g., logistic regression, neural networks).\\n\\n4. **Train-Test Split**:\\n   - The dataset is split into training and testing subsets to evaluate model performance. Testing data is 20% of the overall dataset.\\n\\nThe code demonstrates a common data preprocessing workflow that can be adjusted based on the requirements of your specific dataset or task. If you need clarification or additional preprocessing techniques, feel free to ask!'}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for _id, doc in QA_CACHE.docstore._dict.items():\n",
    "        print(\"ID:\", _id)\n",
    "        print(\"Text:\", doc.page_content)\n",
    "        print(\"Metadata:\", doc.metadata)\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc_id, doc in QA_CACHE.docstore._dict.items():\n",
    "#     print(\"ID:\", doc_id)\n",
    "#     print(\"Text:\", doc.page_content)\n",
    "#     print(\"Metadata:\", doc.metadata[\"topic\"])\n",
    "#     print(\"answer:\", doc.metadata[\"answer\"])\n",
    "#     print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
